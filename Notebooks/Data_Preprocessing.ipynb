{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05de3fe6-3163-4e80-90ee-2ada98904936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang import tl, en\n",
    "from tglstemmer import stemmer\n",
    "import simplemma\n",
    "import string as s\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79d3bec3-da40-4559-a32d-1bac118ad842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3206, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Data/full.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00f331a0-edc4-47d9-a39b-411c4c3100aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ML_USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_tl = tl.stop_words.STOP_WORDS # Importing\n",
    "stopwords_en = en.stop_words.STOP_WORDS\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8069688-e5a4-4f65-9e1e-d5f0bdfa414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CSV file was generated to be used as a checkpoint as the preprocessing method I used took a long time to finish.\n",
    "stemmer_tl = stemmer\n",
    "def word_proc_new(data):\n",
    "    if not isinstance(data, str):\n",
    "        return None  # Mark for removal later\n",
    "\n",
    "    try:\n",
    "        data = data.translate(str.maketrans('', '', s.punctuation))\n",
    "        data = data.lower()\n",
    "\n",
    "        stems = stemmer_tl.get_stems(data)\n",
    "        if not isinstance(stems, list):\n",
    "            print(\"Error at Stemmer!\")\n",
    "            return None \n",
    "\n",
    "        data_stem = []\n",
    "        for word in stems:\n",
    "            if isinstance(word, str) and word:\n",
    "                lemma = simplemma.lemmatize(word, lang='en', greedy=True)\n",
    "                data_stem.append(lemma)\n",
    "\n",
    "        data_sw_tl = [word for word in data_stem if word not in stopwords_tl]\n",
    "        data_sw_en = [word for word in data_sw_tl if word not in stopwords_en]\n",
    "        data_final = \" \".join(data_sw_en)\n",
    "        return data_final\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error, element dropped.\")\n",
    "        return None  # Mark for removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eaa66eb-f2ca-438b-987a-13f09f91d7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n",
      "Error, element dropped.\n"
     ]
    }
   ],
   "source": [
    "X = df.article\n",
    "X_clean = X.apply(word_proc_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6157cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =  train_test_split(X_clean, df.label, test_size=0.2, stratify=df.label, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c7879a9-beac-4b07-8672-cf7a721b5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.concat([X_train,y_train], axis=1)\n",
    "Test = pd.concat([X_train,y_train], axis=1)\n",
    "X_clean.to_csv(\"dataset_preprocessed.csv\")\n",
    "Train.to_csv(\"Data\\\\Train.csv\")\n",
    "Test.to_csv(\"Data\\\\Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b74743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
